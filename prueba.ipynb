{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICCIÓN DE TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un día, soy de tierra, me conecto con la realidad a través del orden y la rutina que me mantienen anclado. Pero muy ocasionalmente, me transformo en aire, me elevo por encima de las limitaciones terrenales y me vuelvo a encontrar con mi espíritu aventurero. Dejo que mi imaginación vuele, explorando mundos lejanos y nuevas posibilidades. Me\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "if not cohere_api_key:\n",
    "    raise ValueError(\"Qué onda la API_KEY. Please set the COHERE_API_KEY environment variable.\")\n",
    "\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "try:\n",
    "    response = co.generate(\n",
    "        model='command-xlarge-nightly',  \n",
    "        prompt=\"Continúa la siguiente frase de manera creativa y fluida: \"+\n",
    "            \"Un día, soy de tierra, me conecto con la realidad desde el \"+\n",
    "            \"mantenimiento del orden y la rutina. Muy ocasionalmente, \"+\n",
    "            \"soy aire, me vuelvo a encontrar con...\",\n",
    "        max_tokens=80,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "    print(response.generations[0].text)\n",
    "\n",
    "except cohere.CohereAPIError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plantillas de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez un bravo trabajador en un país distópico, que se rompía el lomo día tras día en una fábrica opresiva. Su sudor y esfuerzo parecían no tener fin, y a menudo se preguntaba si su arduo trabajo realmente valía la pena.\n",
      "\n",
      "Un día, mientras revisaba su cheque de pago, se dio cuenta de que algo and\n"
     ]
    }
   ],
   "source": [
    "templates = {\n",
    "   \"historia\": (\"Continúa la siguiente historia de manera creativa y fluida: \"\n",
    "                 \"Había una vez un {character} en un {place}, \"\n",
    "                 \"que estaba {activity}. Un día, {event}. \"\n",
    "                 \"{question}\")\n",
    "}\n",
    "\n",
    "selected_template = templates[\"historia\"]\n",
    "prompt = selected_template.format(character=\"bravo trabajador\", \n",
    "                                  place=\"país distópico\", \n",
    "                                  activity=\"rompiéndose el lomo\", \n",
    "                                  event=\"descubrió que estaba siendo estafado\",\n",
    "                                  question=\"¿Tiene final feliz?\")\n",
    "\n",
    "\n",
    "response = co.generate(\n",
    "        model='command-xlarge-nightly',  \n",
    "        prompt=prompt,\n",
    "        max_tokens=80,\n",
    "        temperature=0.7\n",
    "    )\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros (temperatura, k y p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = co.generate(\n",
    "        model='command-xlarge-nightly',  \n",
    "        prompt='Name a country from Oceania',\n",
    "        max_tokens=5,\n",
    "        temperature=0.1,\n",
    "        k=5,\n",
    "        p=0.9\n",
    "    )\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Arrivederci!', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '2d4bede1-f9d7-48cd-a71d-e61ade4ed494', 'token_count': {'input_tokens': 79, 'output_tokens': 4}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '2d4bede1-f9d7-48cd-a71d-e61ade4ed494', 'token_count': {'input_tokens': 79, 'output_tokens': 4}}, id='run-17f8117f-4609-4390-95cd-935e8b61efcf-0', usage_metadata={'input_tokens': 79, 'output_tokens': 4, 'total_tokens': 83})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"goodbye!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arrivederci!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "result = model.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into french:'), HumanMessage(content='cigüeña')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "result = prompt_template.invoke({\"language\": \"french\", \"text\": \"cigüeña\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La traduction de \"cigüeña\" en français est \"cigogne\".'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"language\": \"french\", \"text\": \"cigüeña\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
