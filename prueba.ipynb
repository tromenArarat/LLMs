{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICCIÓN DE TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...la libertad y la creatividad. Soy capaz de volar, de imaginar y de crear sin límites. Me dejo llevar por el viento, por las ideas y por los sueños que me transportan a mundos nuevos y emocionantes. Pero, aunque soy aire, no me olvido de mis raíces, de mi conexión con la tierra, pues es allí donde encuentro la estabilidad y la fuerza para\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "if not cohere_api_key:\n",
    "    raise ValueError(\"No API key found. Please set the COHERE_API_KEY environment variable.\")\n",
    "\n",
    "co = cohere.Client(cohere_api_key)\n",
    "\n",
    "try:\n",
    "    response = co.generate(\n",
    "        model='command-xlarge-nightly',  \n",
    "        prompt=\"Continúa la siguiente frase de manera creativa y fluida: \"+\n",
    "            \"Un día, soy de tierra, me conecto con la realidad desde el \"+\n",
    "            \"mantenimiento del orden y la rutina. En otro tiempo, muy ocasional, \"+\n",
    "            \"soy aire, me vuelvo a encontrar con...\",\n",
    "        max_tokens=80,\n",
    "        temperature=0.9\n",
    "    )\n",
    "\n",
    "    print(response.generations[0].text)\n",
    "\n",
    "except cohere.CohereAPIError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plantillas de prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez un bravo trabajador en un país distópico que, día tras día, se rompía el lomo para salir adelante. Su trabajo era arduo y agotador, pero lo realizaba con dedicación y orgullo. Sin embargo, un día, descubrió una verdad impactante: estaba siendo estafado por su propio gobierno.\n",
      "\n",
      "El trabajador se dio cuenta de que\n"
     ]
    }
   ],
   "source": [
    "templates = {\n",
    "   \"historia\": (\"Continúa la siguiente historia de manera creativa y fluida: \"\n",
    "                 \"Había una vez un {character} en un {place}, \"\n",
    "                 \"que estaba {activity}. Un día, {event}. \"\n",
    "                 \"{question}\")\n",
    "}\n",
    "\n",
    "selected_template = templates[\"historia\"]\n",
    "prompt = selected_template.format(character=\"bravo trabajador\", \n",
    "                                  place=\"país distópico\", \n",
    "                                  activity=\"rompiéndose el lomo\", \n",
    "                                  event=\"descubrió que estaba siendo estafado\",\n",
    "                                  question = \"¿Tiene final feliz?\")\n",
    "\n",
    "\n",
    "response = co.generate(\n",
    "        model='command-xlarge-nightly',  \n",
    "        prompt=prompt,\n",
    "        max_tokens=80,\n",
    "        temperature=0.7\n",
    "    )\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros (temperatura, k y p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Zealand is a country\n"
     ]
    }
   ],
   "source": [
    "response = co.generate(\n",
    "        model='command-xlarge-nightly',  \n",
    "        prompt='Name a country from Oceania',\n",
    "        max_tokens=5,\n",
    "        temperature=0.1,\n",
    "        k=5,\n",
    "        p=0.9\n",
    "    )\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Arrivederci!', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '7a76c2c8-d776-4192-ae24-0ebb1dbd1354', 'token_count': {'input_tokens': 79, 'output_tokens': 4}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '7a76c2c8-d776-4192-ae24-0ebb1dbd1354', 'token_count': {'input_tokens': 79, 'output_tokens': 4}}, id='run-11c6a5a2-d7cd-4287-8ff2-acdfa545e02c-0', usage_metadata={'input_tokens': 79, 'output_tokens': 4, 'total_tokens': 83})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"goodbye!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arrivederci!'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "result = model.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into french:'), HumanMessage(content='cigüeña')])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "result = prompt_template.invoke({\"language\": \"french\", \"text\": \"cigüeña\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La traduction de \"cigüeña\" en français est \"cigogne\".'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"language\": \"french\", \"text\": \"cigüeña\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
